{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TO DO\n",
    "1. Extract the each race URLs from the page year\n",
    "2. Clean the list to get just the URLs with the circuit race\n",
    "3. Extract the race result table date and circuit for each race URL\n",
    "4. Store the tables in a dictionary\n",
    "5. Clean the dictionary dropping the columns with Nan values\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c769ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "import chardet\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9dcd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs to extract\n",
    "\n",
    "\n",
    "#\"https://www.formula1.com/en/results.html/2018/races.html\"\n",
    "#\"https://www.formula1.com/en/results.html/2019/races.html\"\n",
    "#\"https://www.formula1.com/en/results.html/2020/races.html\"\n",
    "#\"https://www.formula1.com/en/results.html/2021/races.html\"\n",
    "#\"https://www.formula1.com/en/results.html/2022/races.html\"\n",
    "#\"https://www.formula1.com/en/results.html/2023/races.html\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9635682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_menu_links(url):\n",
    "    url_list = []  # Initialize an empty list to store the URLs\n",
    "    try:\n",
    "        # Makes a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Checks if the request was successful\n",
    "\n",
    "        #  Creates a BeautifulSoup object from the page content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # Finds the elements <li> with the specific class\n",
    "        li_elements = soup.find_all(\"li\", class_=\"resultsarchive-filter-item\")\n",
    "\n",
    "        if li_elements:\n",
    "            # Extracts and adds the URLs to the list inside the <li> elements\n",
    "            for li_element in li_elements:\n",
    "                # Finds the elements <a> inside each <li>\n",
    "                a_elements = li_element.find_all(\n",
    "                    \"a\",\n",
    "                    href=lambda href: href and href.startswith(\"/en/results.html/2023\"),\n",
    "                )\n",
    "\n",
    "                # Add URLs to the list\n",
    "                for a_element in a_elements:\n",
    "                    href = a_element.get(\"href\")\n",
    "                    if href:\n",
    "                        # Uses urljoin to create URLs\n",
    "                        absolute_url = urljoin(url, href)\n",
    "                        url_list.append(absolute_url)\n",
    "        else:\n",
    "            print(\"Elementos <li> n√£o encontrados.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "\n",
    "    return url_list\n",
    "\n",
    "\n",
    "# URL page with elements <li>\n",
    "#Change URL\n",
    "url = \"https://www.formula1.com/en/results.html/2023/races.html\"\n",
    "\n",
    "# Calls function\n",
    "urls_list = extract_menu_links(url)\n",
    "\n",
    "print(\"Lista de URLs:\", urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96937f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only racing URLs\n",
    "filtered_urls = [url for url in urls_list if '/race-result.html' in url]\n",
    "\n",
    "# Print filtered URLs\n",
    "print(filtered_urls)\n",
    "\n",
    "# Print the length of filtered URLs\n",
    "print(len(filtered_urls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change URL\n",
    "url = 'https://www.formula1.com/en/results.html/2023/races/1141/bahrain/race-result.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed79a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_from_url(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find the table on the page (you might need to adjust this based on the HTML structure)\n",
    "        table = soup.find(\"table\")\n",
    "\n",
    "        # Find the date element within a <p> tag\n",
    "        date_element = soup.find(\"p\", class_=\"date\")\n",
    "\n",
    "        # Find the element <span class=\"full-date\">\n",
    "        full_date_element = date_element.find(\"span\", class_=\"full-date\") if date_element else None\n",
    "\n",
    "        # Find the element <span class=\"circuit-info\">\n",
    "        circuit_element = soup.find(\"span\", class_=\"circuit-info\")\n",
    "\n",
    "        if table:\n",
    "            # Use pandas to read the HTML table into a DataFrame\n",
    "            df = pd.read_html(str(table))[0]\n",
    "\n",
    "            # Add a new column 'Date' with the value of the element <span class=\"full-date\">\n",
    "            if 'Date' in df.columns:\n",
    "                # Update the existing 'Date' column\n",
    "                df['Date'] = full_date_element.get_text() if full_date_element else \"No date element found\"\n",
    "            else:\n",
    "                # Add a new column 'Date' with the value of the element <span class=\"full-date\">\n",
    "                df.insert(0, \"Date\", full_date_element.get_text() if full_date_element else \"No date element found\")\n",
    "\n",
    "\n",
    "            # Add a new column 'Circuit' with the value of the element <span class=\"circuit-info\">\n",
    "            if circuit_element:\n",
    "                df.insert(0, \"Circuit\", circuit_element.get_text())\n",
    "                df[\"Circuit\"] = df[\"Circuit\"].str.replace(',', ' -')  # Replace ',' with '-'\n",
    "            else:\n",
    "                df.insert(0, \"Circuit\", \"No circuit element found\")\n",
    "\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No table found on the page.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "table_data = extract_table_from_url(url)\n",
    "\n",
    "if table_data is not None:\n",
    "    display(table_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e67eed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dict = {}\n",
    "\n",
    "# Loop through each URL\n",
    "for url in filtered_urls:\n",
    "    table_data = extract_table_from_url(url)\n",
    "\n",
    "    if table_data is not None:\n",
    "        # Store the table in the dictionary with the URL as the key\n",
    "        tables_dict[url] = table_data\n",
    "\n",
    "# Concatenate the tables into a single DataFrame\n",
    "result_df = pd.concat(tables_dict.values(), keys=tables_dict.keys())\n",
    "\n",
    "# Display the concatenated DataFrame\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917b9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = result_df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e615bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting date from char to datetime \n",
    "\n",
    "df_cleaned['Date'] = pd.to_datetime(df_cleaned['Date'], format='%d %b %Y')\n",
    "\n",
    "print(df_cleaned.dtypes)\n",
    "display(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc041d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = 'C:/Users/bruno/Projetos Python/F1_Results_Web_Scraping/'\n",
    "\n",
    "# Export DataFrame to CSV file in the specified directory\n",
    "df_cleaned.to_csv(output_directory + 'races_results_2023.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
